{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 3: Policy Search\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The player **taking the last object wins**.\n",
    "\n",
    "* Task3.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task3.2: An agent using evolved rules\n",
    "* Task3.3: An agent using minmax\n",
    "* Task3.4: An agent using reinforcement learning\n",
    "\n",
    "## Deadlines ([AoE](https://en.wikipedia.org/wiki/Anywhere_on_Earth))\n",
    "\n",
    "* Sunday, December 4th for Task3.1 and Task3.2\n",
    "* Sunday, December 11th for Task3.3 and Task3.4\n",
    "* Sunday, December 18th for all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "from operator import xor\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s %(levelname)s: %(message)s\", level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Nim* and *Nimply* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")\n",
    "\n",
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "        self.player = 0\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply     \n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects\n",
    "        self.player = 1 - self.player\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(bytes([*self.rows, self.player]))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return all(map(lambda x: x[0] == x[1], zip(self.rows, other.rows))) and self.player == other.player\n",
    "\n",
    "def nim_sum(state: Nim) -> int:\n",
    "    result = reduce(xor, state.rows)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cook_status(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    return cooked\n",
    "\n",
    "def optimal_strategy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), data[\"brute_force\"][0])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My fixed-rule agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Genome = namedtuple(\"Genome\", \"aggressivity, longest_first, how_many\")\n",
    "Individual = namedtuple(\"Individual\", \"genome, fitness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_strategy(genome: Genome) -> Callable:\n",
    "    def evolvable(state: Nim) -> Nimply:\n",
    "        # data = cook_status(state)\n",
    "        aggressive: bool = random.random() < genome.aggressivity\n",
    "        longest_first: bool = random.random() < genome.longest_first\n",
    "        how_many_coeff: float = genome.how_many\n",
    "\n",
    "        # Sort the rows based on the number of elements, the sort is descending or ascending based on if longest_first or not\n",
    "        row_indexes = sorted((i for i in range(len(state.rows)) if state.rows[i] > 0), key=lambda elem: state.rows[elem], reverse=longest_first)\n",
    "\n",
    "        # Select randomly one of the first 50% rows\n",
    "        selected_row_index = random.choice(row_indexes[:int(0.5*len(row_indexes))+1])\n",
    "\n",
    "        # Decide to take at least 1 or half of the objects\n",
    "        take_at_least = 0 if not aggressive else state.rows[selected_row_index]//2\n",
    "        \n",
    "        # Decide to take or not a part of the remaining objects\n",
    "        take_n = max(1, min(take_at_least + int(state.rows[selected_row_index]//2*how_many_coeff), state.rows[selected_row_index]))\n",
    "        ply = Nimply(selected_row_index, take_n)\n",
    "\n",
    "        return ply\n",
    "\n",
    "    return evolvable\n",
    "\n",
    "# Task 3.1 agent\n",
    "not_evovled = make_strategy(Genome(0.5, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other fixed-rule agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)\n",
    "\n",
    "def gabriele(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the maximum possible number of the lowest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (-m[0], m[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent base policy it's the same of task 3.1, but now I evolve its parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIM_SIZE = 10\n",
    "\n",
    "\n",
    "def evaluate(strategy: Callable, opponent: Callable=optimal_strategy, NUM_MATCHES=100) -> float:\n",
    "    strategies = (strategy, opponent)\n",
    "    won = 0\n",
    "\n",
    "    for m in range(NUM_MATCHES):\n",
    "        nim = Nim(NIM_SIZE)\n",
    "        player = 0\n",
    "        while nim:\n",
    "            ply = strategies[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1:\n",
    "            won += 1\n",
    "    return won / NUM_MATCHES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolve genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament(population, k=10):\n",
    "    return max(random.choices(population, k=k), key=lambda x: x.fitness)\n",
    "\n",
    "def make_offspring(p1: Individual, p2: Individual, current_best_strategy):\n",
    "    new_genome = []\n",
    "    for i in range(len(p1.genome)):\n",
    "        gene = p1.genome[i] if random.random() > 0.5 else p2.genome[i] # inherit from p1 or p2 based on randomness\n",
    "        gene += random.gauss(0, 0.25) # tweak\n",
    "        gene = max(0, min(gene, 1)) # clip to avoid unammissible solutions\n",
    "        new_genome.append(gene)\n",
    "\n",
    "    new_genome = Genome(*new_genome)\n",
    "    individual = Individual(new_genome, evaluate(make_strategy(new_genome), current_best_strategy)) # create individual and compute fitness\n",
    "    \n",
    "    return individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 20\n",
    "POPULATION_SIZE = 40\n",
    "OFFSPRING_SIZE = 200\n",
    "genomes = [Genome(0.5 + random.random()/10, 0.5 + random.random()/10, 0.5 + random.random()/10) for _ in range(POPULATION_SIZE)]\n",
    "\n",
    "# the initial population fitness is computed against gabriele\n",
    "population = list(map(lambda genome: Individual(genome, evaluate(make_strategy(genome), gabriele)), genomes))\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    logging.debug(f\"Starting iteration {i}. Current fitness: {population[0].fitness}\")\n",
    "    logging.debug(f\"Current best:\\nAggressivity\\tLongest first\\tHow many\\n\\\n",
    "{population[0].genome.aggressivity:.2f}\\t\\t{population[0].genome.longest_first:.2f}\\t\\t{population[0].genome.how_many:.2f}\")\n",
    "\n",
    "    offspring = []\n",
    "    for _ in range(OFFSPRING_SIZE):\n",
    "        p1, p2 = tournament(population, k=1), tournament(population, k=1)\n",
    "        o = make_offspring(p1, p2, make_strategy(population[0].genome))\n",
    "        offspring.append(o)\n",
    "    population.extend(offspring)\n",
    "    population = sorted(population, key=lambda individual: individual.fitness, reverse=True)[:POPULATION_SIZE]\n",
    "\n",
    "evolved_individual = population[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 final match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = (make_strategy(evolved_individual.genome), optimal_strategy)\n",
    "\n",
    "nim = Nim(11)\n",
    "logging.debug(f\"status: Initial board  -> {nim}\")\n",
    "player = 0\n",
    "while nim:\n",
    "    ply = strategies[player](nim)\n",
    "    nim.nimming(ply)\n",
    "    logging.debug(f\"status: After player {player} -> {nim}\")\n",
    "    player = 1 - player\n",
    "winner = 1 - player\n",
    "logging.info(f\"status: Player {winner} won!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "import math\n",
    "\n",
    "@cache\n",
    "def possible_actions(state):\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1)]\n",
    "    return possible_moves\n",
    "\n",
    "@cache\n",
    "def result(state: Nim, action: Nimply):\n",
    "    new_state = deepcopy(state)\n",
    "    new_state.nimming(action)\n",
    "    return new_state\n",
    "\n",
    "# https://mathspp.com/blog/minimax-algorithm-and-alpha-beta-pruning\n",
    "\n",
    "def minmax_strategy_ab(state: Nim):\n",
    "    \n",
    "    def minmax(state: Nim, alpha=-1, beta=1):\n",
    "        maximising_player = state.player == 0\n",
    "\n",
    "        if not state:\n",
    "            # no more moves: state.player lost\n",
    "            return None, 1 if not maximising_player else -1\n",
    "\n",
    "        val = (None, -1) if maximising_player else (None, 1)\n",
    "        for ply in cook_status(state)[\"possible_moves\"]:\n",
    "            new_state = result(state, ply)\n",
    "            _, ns_value = minmax(new_state, alpha, beta)\n",
    "\n",
    "            if maximising_player:\n",
    "                val = max((ply, ns_value), val, key=lambda x: x[1])\n",
    "                alpha = max(alpha, ns_value)\n",
    "            else:\n",
    "                val = min((ply, ns_value), val, key=lambda x: x[1])\n",
    "                beta = min(beta, ns_value)\n",
    "\n",
    "            \n",
    "            if (maximising_player and val[1] >= beta) or (not maximising_player and val[1] <= alpha):\n",
    "                break\n",
    "        return val\n",
    "    return minmax(state)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, alpha=0.15, explore_factor=0.8):  # 80% explore, 20% exploit\n",
    "        self.state_history = []  # state, reward\n",
    "        self.alpha = alpha\n",
    "        self.explore_factor = explore_factor\n",
    "        self.G = defaultdict(lambda: np.random.uniform(low=0.1, high=1))\n",
    "\n",
    "    def choose_action(self, state, allowedMoves, train=True):\n",
    "        maxG = -10e15\n",
    "        next_move = None\n",
    "        randomN = np.random.random()\n",
    "\n",
    "        explore_factor = self.explore_factor if train else 0.05\n",
    "\n",
    "        if randomN < explore_factor:\n",
    "            # if random number below random factor, choose random action\n",
    "            # i need to do this since allowedMoves is a list of tuples and it messes up numpy\n",
    "            next_move = allowedMoves[np.random.choice(len(allowedMoves))]\n",
    "        else:\n",
    "            # if exploiting, gather all possible actions and choose one with the highest G (reward)\n",
    "            for action in allowedMoves:\n",
    "                new_state = deepcopy(state)\n",
    "                new_state.nimming(action)\n",
    "                if self.G[new_state] >= maxG:\n",
    "                    next_move = action\n",
    "                    maxG = self.G[new_state]\n",
    "\n",
    "        return next_move\n",
    "\n",
    "    def update_state_history(self, state, reward):\n",
    "        self.state_history.append((state, reward))\n",
    "\n",
    "    def learn(self):\n",
    "        target = 0\n",
    "\n",
    "        for prev, reward in reversed(self.state_history):\n",
    "            self.G[prev] = self.G[prev] + self.alpha * (target - self.G[prev])\n",
    "            target += reward\n",
    "\n",
    "        self.state_history = []\n",
    "\n",
    "        self.explore_factor -= 10e-5  # decrease random factor each episode of play\n",
    "\n",
    "    def __call__(self, state):\n",
    "        allowed_moves = cook_status(state)[\"possible_moves\"]\n",
    "        return self.choose_action(state, allowed_moves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent against optimal\n",
    "GAME_LENGTH = 4\n",
    "fact = lambda n: n*(n-1)\n",
    "robot = Agent()\n",
    "for i in range(100000):\n",
    "    player = 0\n",
    "    nim = Nim(GAME_LENGTH)\n",
    "    while nim:\n",
    "        if player == 1:\n",
    "            ply = optimal_strategy(nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "            continue\n",
    "\n",
    "        action = robot(nim)\n",
    "        nim.nimming(action)\n",
    "        state, reward = nim, -1 if nim else fact(GAME_LENGTH)\n",
    "        robot.update_state_history(state, reward)\n",
    "        player = 1 - player\n",
    "    robot.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 12:43:39,955 DEBUG: status: Initial board  -> <1 3 5 7>\n",
      "2023-03-21 12:43:39,956 DEBUG: status: After player 0 -> <1 2 5 7>\n",
      "2023-03-21 12:43:39,957 DEBUG: status: After player 1 -> <0 2 5 7>\n",
      "2023-03-21 12:43:39,959 DEBUG: status: After player 0 -> <0 2 5 2>\n",
      "2023-03-21 12:43:39,960 DEBUG: status: After player 1 -> <0 2 0 2>\n",
      "2023-03-21 12:43:39,961 DEBUG: status: After player 0 -> <0 1 0 2>\n",
      "2023-03-21 12:43:39,962 DEBUG: status: After player 1 -> <0 1 0 1>\n",
      "2023-03-21 12:43:39,963 DEBUG: status: After player 0 -> <0 1 0 0>\n",
      "2023-03-21 12:43:39,964 DEBUG: status: After player 1 -> <0 0 0 0>\n",
      "2023-03-21 12:43:39,964 INFO: status: Player 1 won!\n"
     ]
    }
   ],
   "source": [
    "strategies = (robot, optimal_strategy)\n",
    "\n",
    "nim = Nim(4)\n",
    "logging.debug(f\"status: Initial board  -> {nim}\")\n",
    "player = 0\n",
    "while nim:\n",
    "    ply = strategies[player](nim)\n",
    "    nim.nimming(ply)\n",
    "    logging.debug(f\"status: After player {player} -> {nim}\")\n",
    "    player = 1 - player\n",
    "winner = 1 - player\n",
    "logging.info(f\"status: Player {winner} won!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
